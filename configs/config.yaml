
wandb:
  project: "osg-testing"
  log_to_wandb: false

training:
  n_total_train_steps: 1000000
  n_steps_per_epoch: 1000 
  eval_freq: 10 
  learning_rate: 0.001
  batch_size: 64
  save_checkpoints: false 
  log_freq: 100
  save_freq: 100000
  gradient_accumulate_every: 2
  device: "cuda:0"
  load_path: "home/path_to_load"

diffusion:
  n_diffusion_steps: 100 
  clip_denoised: true
  predict_epsilon: true
  loss_discount: 1.0 
  loss_type: "state_l2"

model:
  type: "random"
  model_specific_param: 1.0  

dataset:
  env_name: "halfcheetah-expert-v2"
  normalizer_name: "CDFNormalizer" # type of normalizer used in data process
  horizon: 100  # planning horizon 
  max_episode_len: 1000 # max len for each trajectory 
  max_n_episodes: 10000 # max size for trajectories in the replay buffer 
  termination_penalty: -100.0 # penalty for early termination in the trajectory TODO: do we need this ?  
  use_padding: False

target:
  target_percentile: 90  # target percentile in terms of reward
  target_len: 20  # len of target observation sequence 
  known_obs_len: 20 