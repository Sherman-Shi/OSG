
wandb:
  project: "osg-testing"
  log_to_wandb: true

training:
  train_dynamic: true
  train_target: true
  n_total_train_steps: 1000000
  n_steps_per_epoch: 1000 
  eval_freq: 10 
  learning_rate: 0.0002
  batch_size: 64
  save_checkpoints: false 
  log_freq: 10
  save_freq: 100 #save for every * epoch 
  gradient_accumulate_every: 2
  device: "cuda:1"
  load_checkpoint: false 
  load_path: "/home/zhenpeng/桌面/brainstorm/OSG/weights/halfcheetah-expert-v2_checkpoint/state_499.pt"
  load_target_checkpoint: false
  load_target_path: "/home/zhenpeng/桌面/brainstorm/OSG/weights/halfcheetah-expert-v2_checkpoint/target_state_499.pt"

diffusion:
  n_diffusion_steps: 100 
  clip_denoised: true
  predict_epsilon: true
  loss_discount: 1.0 
  loss_type: "state_l2" 

model:
  type: "osg"
  model_specific_param: 1.0  

dataset:
  env_name: "halfcheetah-expert-v2"
  normalizer_name: "CDFNormalizer" # type of normalizer used in data process
  horizon: 128  # planning horizon 
  max_episode_len: 1000 # max len for each trajectory 
  max_n_episodes: 10000 # max size for trajectories in the replay buffer 
  termination_penalty: -100.0 # penalty for early termination in the trajectory TODO: do we need this ?  
  use_padding: False

target:
  target_percentile: 90  # target percentile in terms of reward
  target_len: 24  # len of target observation sequence 
  known_obs_len: 24 # len of known observations or history 